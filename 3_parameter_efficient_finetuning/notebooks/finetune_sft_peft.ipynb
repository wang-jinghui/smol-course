{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-6LLOPZouLg"
   },
   "source": [
    "# 如何使用TRL通过LoRA适配器微调大型语言模型（LLMs）\n",
    "\n",
    "This notebook demonstrates how to efficiently fine-tune large language models using LoRA (Low-Rank Adaptation) adapters. LoRA is a parameter-efficient fine-tuning technique that:\n",
    "本笔记本展示了如何使用LoRA（低秩适应）适配器高效地微调大型语言模型。LoRA是一种参数高效的微调技术，具有以下特点：本笔记本展示了如何使用LoRA（低秩适应）适配器高效地微调大型语言模型。LoRA是一种参数高效的微调技术，具有以下特点：\n",
    "- 冻结预训练模型的权重\n",
    "- 在注意力层中添加小的可训练秩分解矩阵\n",
    "- 通常将可训练参数减少约90%\n",
    "- 在保持模型性能的同时提高内存效率\n",
    "\n",
    "我们将涵盖以下内容：\n",
    "- 设置开发环境和LoRA配置\n",
    "- 创建并准备适配器训练所需的数据集\n",
    "- 使用带有LoRA适配器的trl和SFTTrainer进行微调\n",
    "- 测试模型并合并适配器（可选）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXqd9BXgouLi"
   },
   "source": [
    "## 1. 设置开发环境\n",
    "\n",
    "我们的第一步是安装Hugging Face库和Pytorch，包括trl、transformers和datasets。如果你还没听说过trl，不用担心。它是一个建立在transformers和datasets之上的新库，使得微调、rlhf（基于人类反馈的强化学习）以及开放大型语言模型的对齐变得更加容易。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKvGVxImouLi"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers datasets trl huggingface_hub\n",
    "# Authenticate to Hugging Face(optional)\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHUzfwpKouLk"
   },
   "source": [
    "## 2. 加载数据集\n",
    "由于网络原因这里选择加载提前下载好的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z4p6Bvo7ouLk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wjh/anaconda3/envs/slm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 2260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# define your dataset and config using the path and name parameters\n",
    "dataset = load_dataset(\"parquet\", data_files={'train': '/dataset/smoltalk/everyday-conversations/train-00000-of-00001.parquet',\n",
    "                                              'test': '/dataset/smoltalk/everyday-conversations/test-00000-of-00001.parquet'})\n",
    "#dataset = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Hey!', 'role': 'user'},\n",
       " {'content': 'Hello! How can I help you today?', 'role': 'assistant'},\n",
       " {'content': \"I'm planning a trip to Paris. What are some popular tourist attractions?\",\n",
       "  'role': 'user'},\n",
       " {'content': 'The Eiffel Tower, the Louvre Museum, and Notre Dame Cathedral are must-visit places in Paris.',\n",
       "  'role': 'assistant'},\n",
       " {'content': 'That sounds great. Are there any local markets I should check out?',\n",
       "  'role': 'user'},\n",
       " {'content': 'Yes, the Champs-Élysées Christmas Market and the Marché aux Puces de Saint-Ouen (flea market) are very popular among tourists and locals alike.',\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Awesome, thank you for the recommendations!', 'role': 'user'},\n",
       " {'content': \"You're welcome! Have a great time in Paris!\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['messages'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TOhJdtsouLk"
   },
   "source": [
    "## 3. 使用trl和带有LoRA的SFTTrainer微调LLM\n",
    "\n",
    "trl中的[SFTTrainer](https://huggingface.co/docs/trl/sft_trainer)通过[PEFT](https://huggingface.co/docs/peft/en/index)库提供了与LoRA适配器的集成。这种设置的主要优势包括：\n",
    "\n",
    "1.**内存效率**：\n",
    "- 仅适配器参数存储在GPU内存中\n",
    "- 基础模型权重保持冻结，并且可以以较低的精度加载\n",
    "- 能够在消费者级GPU上对大型模型进行微调\n",
    "\n",
    "2.**训练功能**：\n",
    "- 与PEFT/LoRA的原生集成，设置简便\n",
    "- 支持QLoRA（量化LoRA），以实现更高的内存效率\n",
    "\n",
    "3**适配器管理**：\n",
    "- 在检查点期间保存适配器权重\n",
    "- 具有将适配器合并回基础模型的功能\n",
    "\n",
    "在我们的示例中，我们将使用LoRA，它将LoRA与4位量化相结合，以进一步减少内存使用，同时不牺牲性能。设置仅需要几个配置步骤：\n",
    "1. 定义LoRA配置（rank、alpha、dropout）\n",
    "2. 使用PEFT配置创建SFTTrainer\n",
    "3. 训练并保存适配器权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 加载模型SmolLM2-135M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "# 从本地路径加载模型\n",
    "model_path = \"/models/SmolLM2-135M/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_path\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_path)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-LoRA\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 测试基础模型生成能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "user\n",
      "how are you\n",
      "how are you\n",
      "how are you\n",
      "how are you\n",
      "how are you\n",
      "how are you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"how are you\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  LoRA微调参数\n",
    "SFTTrainer 支持与 peft 的原生集成，这使得使用例如 LoRA 等工具高效调整大型语言模型（LLMs）变得非常简单。我们只需创建自己的 LoraConfig 并将其提供给训练器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "blDSs9swouLk"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: Configure LoRA parameters\n",
    "# r: LoRA更新矩阵的秩（越小表示压缩程度越高）\n",
    "rank_dimension = 6\n",
    "# lora_alpha: LoRA层的缩放因子（越高表示适应能力越强）\n",
    "lora_alpha = 8\n",
    "# lora_dropout: LoRA层的丢弃概率（有助于防止过拟合）”\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5NUDPcaouLl"
   },
   "source": [
    "在开始训练之前，我们需要定义我们想要使用的超参数（TrainingArguments）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NqT28VZlouLl"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "# Hyperparameters based on QLoRA paper recommendations\n",
    "args = SFTConfig(\n",
    "    # Output settings\n",
    "    output_dir=finetune_name,  # Directory to save model checkpoints\n",
    "    # Training duration\n",
    "    num_train_epochs=20,  # Number of training epochs\n",
    "    # Batch size settings\n",
    "    per_device_train_batch_size=8,  # Batch size per GPU\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients for larger effective batch\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,  # Trade compute for memory savings\n",
    "    # Optimizer settings\n",
    "    optim=\"adamw_torch_fused\",  # Use fused AdamW for efficiency\n",
    "    learning_rate=2e-4,  # Learning rate (QLoRA paper)\n",
    "    max_grad_norm=0.3,  # Gradient clipping threshold\n",
    "    # Learning rate schedule\n",
    "    warmup_ratio=0.03,  # Portion of steps for warmup\n",
    "    lr_scheduler_type=\"constant\",  # Keep learning rate constant after warmup\n",
    "    # Logging and saving\n",
    "    logging_steps=10,  # Log metrics every N steps\n",
    "    save_strategy=\"epoch\",  # Save checkpoint every epoch\n",
    "    eval_strategy=\"steps\",          # Evaluate the model at regular intervals\n",
    "    eval_steps=18,                 # Frequency of evaluation\n",
    "    # Precision settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    # Integration settings\n",
    "    push_to_hub=False,  # Don't push to HuggingFace Hub\n",
    "    report_to=\"none\",  # Disable external logging\n",
    "    packing=True,      # Enable input packing for efficiency\n",
    "    max_seq_length=1024,  # # max sequence length for model and packing of the dataset\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # Special tokens handled by template\n",
    "        \"append_concat_token\": False,  # No additional separator needed\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGhR7uFBouLl"
   },
   "source": [
    "现在我们已经拥有了创建SFTTrainer所需的所有构建块，可以开始训练我们的模型了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M00Har2douLl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 428 examples [00:00, 613.97 examples/s]\n",
      "Generating train split: 22 examples [00:00, 494.96 examples/s]\n",
      "WARNING:accelerate.utils.other:Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Create SFTTrainer with LoRA configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,  # LoRA configuration\n",
    "    processing_class=tokenizer,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ_kRN24ouLl"
   },
   "source": [
    "通过在我们的Trainer实例上调用train()方法来开始训练我们的模型。由于我们使用的是PEFT方法，因此我们将只保存调整后的模型权重，而不是完整的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Tq4nIYqKouLl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [260/260 43:42, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.998700</td>\n",
       "      <td>1.840328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.838900</td>\n",
       "      <td>1.652277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.651700</td>\n",
       "      <td>1.519168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.519200</td>\n",
       "      <td>1.437518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.453200</td>\n",
       "      <td>1.382166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.416500</td>\n",
       "      <td>1.340789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.306000</td>\n",
       "      <td>1.307742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.330500</td>\n",
       "      <td>1.283632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.242600</td>\n",
       "      <td>1.265945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.274800</td>\n",
       "      <td>1.251780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.279200</td>\n",
       "      <td>1.239839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>1.252200</td>\n",
       "      <td>1.229582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>1.240800</td>\n",
       "      <td>1.222471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>1.217700</td>\n",
       "      <td>1.215433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C309KsXjouLl"
   },
   "source": [
    "## 5.将LoRA适配器合并到原始模型中\n",
    "\n",
    "在使用LoRA时，我们只训练适配器权重，同时保持基础模型不变。在训练过程中，我们只保存这些轻量级的适配器权重（约2-10MB），而不是保存完整的模型副本。然而，在部署时，您可能希望将适配器重新合并到基础模型中，原因如下：\n",
    "\n",
    "- 简化部署：只需一个模型文件，而不是基础模型加适配器的组合。\n",
    "- 推理速度：无需承担适配器的计算开销。\n",
    "- 框架兼容性：与服务框架的兼容性更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True)\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(args.output_dir, safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yO6E9quouLl"
   },
   "source": [
    "## 6. 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "I5B494OdouLl"
   },
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "P1UhohVdouLl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_name)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(finetune_name, \n",
    "                                                 device_map=\"auto\", \n",
    "                                                 torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer, device=device, max_length=32, temperature=0, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99uFDAuuouLl"
   },
   "source": [
    "Lets test some prompt samples and see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-shSmUbvouLl",
    "outputId": "16d97c61-3b31-4040-c780-3c4de75c3824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    prompt:\n",
      "What is the capital of Germany?\n",
      "    response:\n",
      "The capital of Germany is Berlin.\n",
      "user\n",
      "Hello\n",
      "--------------------------------------------------\n",
      "    prompt:\n",
      "What is the capital of China?\n",
      "    response:\n",
      "The capital of China is Beijing, located in the northern part of the country.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"What is the capital of Germany?\",\n",
    "    \"What is the capital of China?\"\n",
    "]\n",
    "\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:slm]",
   "language": "python",
   "name": "conda-env-slm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
