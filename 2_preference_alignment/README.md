# Preference Alignmentï¼ˆåå¥½å¯¹é½ï¼‰

æ­¤æ¨¡å—æ¶µç›–äº†å°†è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½è¿›è¡Œå¯¹é½çš„æŠ€æœ¯ã€‚è™½ç„¶ç›‘ç£å¾®è°ƒæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ ä»»åŠ¡ï¼Œä½†åå¥½å¯¹é½åˆ™é¼“åŠ±è¾“å‡ºä¸äººç±»æœŸæœ›å’Œä»·å€¼è§‚ç›¸åŒ¹é…ã€‚

## Overview
å…¸å‹çš„å¯¹é½æ–¹æ³•æ¶‰åŠå¤šä¸ªé˜¶æ®µï¼š
- ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼šä½¿æ¨¡å‹é€‚åº”ç‰¹å®šé¢†åŸŸ
- åå¥½å¯¹é½ï¼ˆå¦‚RLHFæˆ–DPOï¼‰ï¼šæé«˜å“åº”è´¨é‡

åƒORPOè¿™æ ·çš„æ›¿ä»£æ–¹æ³•å°†æŒ‡ä»¤è°ƒä¼˜å’Œåå¥½å¯¹é½ç»“åˆä¸ºä¸€ä¸ªè¿‡ç¨‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨DPOå’ŒORPOç®—æ³•ã€‚

å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºä¸åŒå¯¹é½æŠ€æœ¯çš„ä¿¡æ¯ï¼Œå¯ä»¥åœ¨[Argilla Blog](https://argilla.io/blog/mantisnlp-rlhf-part-8)ä¸Šé˜…è¯»æ›´å¤šç›¸å…³å†…å®¹ã€‚

### 1ï¸âƒ£ Direct Preference Optimization (DPO)

ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰é€šè¿‡ç›´æ¥ä½¿ç”¨åå¥½æ•°æ®ä¼˜åŒ–æ¨¡å‹æ¥ç®€åŒ–åå¥½å¯¹é½ã€‚è¿™ç§æ–¹æ³•æ— éœ€å•ç‹¬çš„å¥–åŠ±æ¨¡å‹å’Œå¤æ‚çš„å¼ºåŒ–å­¦ä¹ ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ›´ä¸ºç¨³å®šå’Œé«˜æ•ˆã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ç›´æ¥åå¥½ä¼˜åŒ–[Direct Preference Optimization (DPO) documentation](./dpo.md)æ–‡æ¡£ã€‚

### 2ï¸âƒ£ Odds Ratio Preference Optimization (ORPO)

ORPOåœ¨å•ä¸ªè¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸€ç§å°†æŒ‡ä»¤è°ƒä¼˜å’Œåå¥½å¯¹é½ç›¸ç»“åˆçš„æ–¹æ³•ã€‚å®ƒé€šè¿‡åœ¨æ ‡è®°çº§åˆ«ä¸Šå°†è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ä¸æ¯”ç‡é¡¹ç›¸ç»“åˆï¼Œä¿®æ”¹äº†æ ‡å‡†è¯­è¨€å»ºæ¨¡ç›®æ ‡ã€‚è¯¥æ–¹æ³•å…·æœ‰ç»Ÿä¸€çš„å•é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ã€å‚è€ƒmodel-freeæ¶æ„ä»¥åŠæ”¹è¿›çš„è®¡ç®—æ•ˆç‡ã€‚ORPOåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œåœ¨AlpacaEvalä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ¯”ç‡åå¥½ä¼˜åŒ–ï¼ˆORPOï¼‰æ–‡æ¡£[Odds Ratio Preference Optimization (ORPO) documentation](./orpo.md)ã€‚

## Exercise Notebooks

| Title | Description | Exercise | Link | Colab |
|-------|-------------|----------|------|-------|
| DPO Training | å­¦ä¹ å¦‚ä½•ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–æ¥è®­ç»ƒæ¨¡å‹ | ğŸ¢ Train a model using the Anthropic HH-RLHF dataset<br>ğŸ• Use your own preference dataset<br>ğŸ¦ Experiment with different preference datasets and model sizes | [Notebook](./notebooks/dpo_finetuning_example.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/2_preference_alignment/notebooks/dpo_finetuning_example.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
| ORPO Training | å­¦ä¹ å¦‚ä½•ä½¿ç”¨æ¯”ç‡åå¥½ä¼˜åŒ–æ¥è®­ç»ƒæ¨¡å‹ | ğŸ¢ Train a model using instruction and preference data<br>ğŸ• Experiment with different loss weightings<br>ğŸ¦ Compare ORPO results with DPO | [Notebook](./notebooks/orpo_finetuning_example.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/2_preference_alignment/notebooks/orpo_finetuning_example.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |


## Resources

- [TRL Documentation](https://huggingface.co/docs/trl/index) - Documentation for the Transformers Reinforcement Learning (TRL) library, which implements various alignment techniques including DPO.
- [DPO Paper](https://arxiv.org/abs/2305.18290) - Original research paper introducing Direct Preference Optimization as a simpler alternative to RLHF that directly optimizes language models using preference data.
- [ORPO Paper](https://arxiv.org/abs/2403.07691) - Introduces Odds Ratio Preference Optimization, a novel approach that combines instruction tuning and preference alignment in a single training stage.
- [Argilla RLHF Guide](https://argilla.io/blog/mantisnlp-rlhf-part-8/) - A guide explaining different alignment techniques including RLHF, DPO, and their practical implementations.
- [Blog post on DPO](https://huggingface.co/blog/dpo-trl) - Practical guide on implementing DPO using the TRL library with code examples and best practices.
- [TRL example script on DPO](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py) - Complete example script demonstrating how to implement DPO training using the TRL library.
- [TRL example script on ORPO](https://github.com/huggingface/trl/blob/main/examples/scripts/orpo.py) - Reference implementation of ORPO training using the TRL library with detailed configuration options.
- [Hugging Face Alignment Handbook](https://github.com/huggingface/alignment-handbook) - Resource guides and codebase for aligning language models using various techniques including SFT, DPO, and RLHF.
